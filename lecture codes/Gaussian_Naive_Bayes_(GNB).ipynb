{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "CLsXp0W6j3pY"
      },
      "outputs": [],
      "source": [
        "from sklearn import datasets\n",
        "from sklearn import metrics\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gaussian Naive Bayes (GNB) is a variant of the Naive Bayes classifier that assumes the likelihood of features to be Gaussian, meaning that they follow a normal distribution. Here's how Gaussian Naive Bayes classification works:\n",
        "\n",
        "### 1. Naive Bayes Algorithm Overview:\n",
        "Naive Bayes is a probabilistic classifier based on Bayes' theorem, which calculates the probability of a class given a set of features. The \"naive\" assumption in Naive Bayes is that the features are conditionally independent given the class, meaning that the presence of one feature does not affect the presence of another feature.\n",
        "\n",
        "### 2. Gaussian Naive Bayes:\n",
        "In Gaussian Naive Bayes, it is assumed that the continuous features in the dataset follow a Gaussian (normal) distribution. This assumption simplifies the calculation of the likelihood probabilities.\n",
        "\n",
        "### 3. Training Phase:\n",
        "During the training phase, GNB estimates the parameters of the Gaussian distribution (mean and variance) for each feature in each class. Specifically:\n",
        "- For each class, GNB calculates the mean and variance of each feature based on the training data belonging to that class.\n",
        "- These parameters are used to model the likelihood of each feature given the class.\n",
        "\n",
        "### 4. Prediction Phase:\n",
        "During the prediction phase, GNB calculates the posterior probability of each class given the features of a new instance using Bayes' theorem:\n",
        "\\[ P(C_k | \\mathbf{x}) = \\frac{P(C_k) \\times P(\\mathbf{x} | C_k)}{P(\\mathbf{x})} \\]\n",
        "\n",
        "Where:\n",
        "- \\( P(C_k | \\mathbf{x}) \\) is the posterior probability of class \\( C_k \\) given the features \\( \\mathbf{x} \\) of the instance.\n",
        "- \\( P(C_k) \\) is the prior probability of class \\( C_k \\).\n",
        "- \\( P(\\mathbf{x} | C_k) \\) is the likelihood of the features \\( \\mathbf{x} \\) given class \\( C_k \\), which is calculated using the Gaussian probability density function.\n",
        "- \\( P(\\mathbf{x}) \\) is the evidence probability, which serves as a normalization factor.\n",
        "\n",
        "### 5. Prediction:\n",
        "- GNB selects the class with the highest posterior probability as the predicted class for the new instance.\n",
        "\n",
        "### 6. Advantages of Gaussian Naive Bayes:\n",
        "- GNB is computationally efficient and scales well to large datasets.\n",
        "- It works well with high-dimensional data and is less prone to overfitting.\n",
        "- Despite its \"naive\" assumption, GNB can perform surprisingly well in practice, especially when the independence assumption holds reasonably well or when the features are correlated but the class conditional distributions are still close to Gaussian.\n",
        "\n",
        "### 7. Limitations:\n",
        "- The assumption of Gaussian distribution may not hold true for all datasets, especially if the features are not continuous or do not follow a normal distribution.\n",
        "- The \"naive\" assumption of feature independence may not be valid in all cases and can lead to suboptimal performance if violated.\n",
        "\n",
        "In summary, Gaussian Naive Bayes classification works by estimating the parameters of Gaussian distributions for each class and each feature during training, and then using these distributions to calculate the posterior probability of each class given the features of a new instance during prediction. It's a simple yet powerful classifier, especially suitable for datasets with continuous features."
      ],
      "metadata": {
        "id": "nQzc51l-lEC0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "iris = datasets.load_iris()\n",
        "X = iris.data\n",
        "Y = iris.target\n",
        "X_train , X_test , Y_train , Y_test = train_test_split(X,Y)"
      ],
      "metadata": {
        "id": "UiJBFyvmkPCN"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = GaussianNB()\n",
        "model.fit(X_train,Y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "BbWcqABYkZhJ",
        "outputId": "fddb7836-7cd8-4b88-daaa-c8fb48c77c86"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GaussianNB()"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GaussianNB()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GaussianNB</label><div class=\"sk-toggleable__content\"><pre>GaussianNB()</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predicted = model.predict(X_test)\n",
        "expected = Y_test\n",
        "metrics.accuracy_score(expected, predicted)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lvSqVgG4k17d",
        "outputId": "c4d00471-2515-4d17-ccad-a5c92aead9b2"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9210526315789473"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's illustrate Gaussian Naive Bayes classification with a simple numerical example. Suppose we have a dataset with two continuous features (X1 and X2) and two classes (Class 0 and Class 1). We'll assume that the features follow a Gaussian distribution within each class.\n",
        "\n",
        "### Dataset:\n",
        "```\n",
        "Class 0:\n",
        "(1, 1), (1, 2), (2, 1)\n",
        "\n",
        "Class 1:\n",
        "(4, 4), (4, 5), (5, 4)\n",
        "```\n",
        "\n",
        "### Training Phase:\n",
        "1. **Calculate Mean and Variance:**\n",
        "   - For each feature (X1 and X2) and each class, calculate the mean and variance.\n",
        "\n",
        "```\n",
        "Class 0:\n",
        "  Mean(X1) = (1+1+2)/3 = 4/3\n",
        "  Variance(X1) = ((1-4/3)^2 + (1-4/3)^2 + (2-4/3)^2)/3 = 1/3\n",
        "  Mean(X2) = (1+2+1)/3 = 4/3\n",
        "  Variance(X2) = ((1-4/3)^2 + (2-4/3)^2 + (1-4/3)^2)/3 = 1/3\n",
        "\n",
        "Class 1:\n",
        "  Mean(X1) = (4+4+5)/3 = 13/3\n",
        "  Variance(X1) = ((4-13/3)^2 + (4-13/3)^2 + (5-13/3)^2)/3 = 1/3\n",
        "  Mean(X2) = (4+5+4)/3 = 13/3\n",
        "  Variance(X2) = ((4-13/3)^2 + (5-13/3)^2 + (4-13/3)^2)/3 = 1/3\n",
        "```\n",
        "\n",
        "### Prediction Phase:\n",
        "Suppose we have a new instance with features (3, 3).\n",
        "\n",
        "1. **Calculate Likelihood:**\n",
        "   - For each class, calculate the likelihood of the features using the Gaussian probability density function.\n",
        "\n",
        "```\n",
        "Class 0:\n",
        "  Likelihood(X1=3) = (1/sqrt(2π*1/3)) * exp(-((3-4/3)^2)/(2*1/3)) ≈ 0.7979\n",
        "  Likelihood(X2=3) = (1/sqrt(2π*1/3)) * exp(-((3-4/3)^2)/(2*1/3)) ≈ 0.7979\n",
        "  P(X|Class 0) ≈ 0.7979 * 0.7979 ≈ 0.6365\n",
        "\n",
        "Class 1:\n",
        "  Likelihood(X1=3) = (1/sqrt(2π*1/3)) * exp(-((3-13/3)^2)/(2*1/3)) ≈ 0.0912\n",
        "  Likelihood(X2=3) = (1/sqrt(2π*1/3)) * exp(-((3-13/3)^2)/(2*1/3)) ≈ 0.0912\n",
        "  P(X|Class 1) ≈ 0.0912 * 0.0912 ≈ 0.0083\n",
        "```\n",
        "\n",
        "2. **Calculate Prior:**\n",
        "   - Suppose equal prior probabilities for each class: \\( P(Class 0) = P(Class 1) = 0.5 \\).\n",
        "\n",
        "3. **Calculate Posterior:**\n",
        "   - Using Bayes' theorem, calculate the posterior probability of each class.\n",
        "\n",
        "```\n",
        "P(Class 0|X) ≈ (0.5 * 0.6365) / ((0.5 * 0.6365) + (0.5 * 0.0083)) ≈ 0.987\n",
        "P(Class 1|X) ≈ (0.5 * 0.0083) / ((0.5 * 0.6365) + (0.5 * 0.0083)) ≈ 0.013\n",
        "```\n",
        "\n",
        "### Prediction:\n",
        "Since \\( P(Class 0|X) > P(Class 1|X) \\), the model predicts Class 0 for the new instance (3, 3).\n",
        "\n",
        "In this example, we trained a Gaussian Naive Bayes classifier using a simple dataset with two classes and two continuous features. We then used the trained classifier to predict the class of a new instance based on its features."
      ],
      "metadata": {
        "id": "M9uRG8AllJ3f"
      }
    }
  ]
}